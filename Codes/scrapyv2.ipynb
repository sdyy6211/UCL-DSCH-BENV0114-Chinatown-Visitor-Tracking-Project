{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the scarping codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'E:\\UCL\\BENV0114\\CHINATOWN\\alldata' ### customize this path to excute following codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### firstly define functions which will be used in scraping.\n",
    "### those are scraping for the whole comments and monthly comments seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url,driver,num_refresh):\n",
    "    driver.get(url = url)\n",
    "\n",
    "    body = driver.find_element_by_xpath('/html')\n",
    "\n",
    "    time.sleep(3+np.random.rand()*2)\n",
    "    \n",
    "    print('finding \"see all reviews\"')\n",
    "    count = 0\n",
    "    while 1:\n",
    "        ### simulate user's behaviour to pagedown\n",
    "        actions_pagedown = ActionChains(driver)\n",
    "        actions_pagedown.move_to_element(body)\n",
    "        actions_pagedown.send_keys(Keys.PAGE_DOWN)\n",
    "        actions_pagedown.perform()\n",
    "        time.sleep(0.5)\n",
    "        try:\n",
    "            ### click See all reviews\n",
    "            fe = driver.find_element_by_xpath(\"//*[contains(text(), 'See all reviews')]\")\n",
    "            actions_click = ActionChains(driver)\n",
    "            actions_click.move_to_element(fe)\n",
    "            actions_click.click()\n",
    "            actions_click.perform()\n",
    "            print('successfully found')\n",
    "            break\n",
    "        except:\n",
    "            if count >= 30:\n",
    "                print('failed to find')\n",
    "                break\n",
    "            else:\n",
    "                print('retry')\n",
    "                count += 1\n",
    "                continue\n",
    "                \n",
    "    ### simulate user's behaviour to load new lada\n",
    "            \n",
    "    pane = driver.find_element_by_xpath('//*[@id=\"pane\"]/div')\n",
    "    \n",
    "    actions_tab = ActionChains(driver)\n",
    "    actions_tab.move_to_element(pane)\n",
    "    actions_tab.send_keys(Keys.TAB)\n",
    "    actions_tab.send_keys(Keys.TAB)\n",
    "    actions_tab.perform()\n",
    "    actions_pagedown_ = ActionChains(driver)\n",
    "    actions_pagedown_.send_keys(Keys.PAGE_DOWN)\n",
    "    \n",
    "    print('loading data')\n",
    "    ### randomly pause to avoid to be detected as a robot\n",
    "    for i in range(num_refresh):\n",
    "        dice = np.random.rand()\n",
    "        time.sleep(dice*3)\n",
    "        actions_pagedown_.perform()\n",
    "        if dice <0.2:\n",
    "            time.sleep(dice*random.choice(range(4,8)))\n",
    "    \n",
    "    return driver.page_source # return the html file to be decoded by the next function\n",
    "\n",
    "def decode_html(page_source):\n",
    "    soup = BeautifulSoup(page_source,'html.parser')\n",
    "    \n",
    "    res_stars = soup.find_all(class_ = 'section-review-stars')#find ratings\n",
    "    \n",
    "    res = soup.find_all(class_ = 'section-review-text')#find comments\n",
    "    \n",
    "    data = pd.DataFrame(columns=['text','label'])\n",
    "\n",
    "    text = []\n",
    "    \n",
    "    jstcache = res[0].attrs['jstcache'] \n",
    "    for i in res:\n",
    "\n",
    "        if i.attrs['jstcache'] == jstcache: ### avoid save replies as comments\n",
    "            \n",
    "            if len(i.contents) > 0:\n",
    "                text.append(i.contents[0])\n",
    "            else:text.append(' ')\n",
    "\n",
    "    labels = []\n",
    "    for i in res_stars:\n",
    "        labels.append(i.attrs['aria-label'][1])\n",
    "\n",
    "    assert(len(text)==len(labels)) ### ensure the length of comments and ratings are matched\n",
    "\n",
    "    data.loc[:,'text'] = text\n",
    "\n",
    "    data.loc[:,'label'] = labels\n",
    "        \n",
    "    return data\n",
    "\n",
    "#### functions below are scraping for monthly comments, but the logics are the same\n",
    "\n",
    "def scrape_month(url,driver,pi):\n",
    "\n",
    "    driver.get(url = url)\n",
    "\n",
    "    body = driver.find_element_by_xpath('/html')\n",
    "\n",
    "    time.sleep(3+np.random.rand()*2)\n",
    "\n",
    "    print('finding \"see all reviews\"')\n",
    "    count = 0\n",
    "    while 1:\n",
    "        actions_pagedown = ActionChains(driver)\n",
    "        actions_pagedown.move_to_element(body)\n",
    "        actions_pagedown.send_keys(Keys.PAGE_DOWN)\n",
    "        actions_pagedown.perform()\n",
    "        time.sleep(0.5)\n",
    "        try:\n",
    "            fe = driver.find_element_by_xpath(\"//*[contains(text(), 'See all reviews')]\")\n",
    "            actions_click = ActionChains(driver)\n",
    "            actions_click.move_to_element(fe)\n",
    "            actions_click.click()\n",
    "            actions_click.perform()\n",
    "            print('successfully found')\n",
    "            break\n",
    "        except:\n",
    "            if count >= 3:\n",
    "                print('failed to find')\n",
    "                break\n",
    "            else:\n",
    "                print('retry')\n",
    "                count += 1\n",
    "                continue\n",
    "    pane = driver.find_element_by_xpath('//*[@id=\"pane\"]/div')\n",
    "    time.sleep(0.2)\n",
    "    driver.find_elements_by_xpath(\"//*[contains(text(), 'Sort')]\")[0].click()\n",
    "    time.sleep(0.2)\n",
    "    driver.find_elements_by_xpath(\"//*[contains(text(), 'Newest')]\")[0].click()\n",
    "    \n",
    "    actions_tab = ActionChains(driver)\n",
    "    actions_tab.move_to_element(pane)\n",
    "    actions_tab.send_keys(Keys.TAB)\n",
    "    actions_tab.send_keys(Keys.TAB)\n",
    "    actions_tab.perform()\n",
    "    actions_pagedown_ = ActionChains(driver)\n",
    "    actions_pagedown_.send_keys(Keys.PAGE_DOWN)\n",
    "    \n",
    "    print('loading data')\n",
    "    while 1:\n",
    "        dice = np.random.rand()\n",
    "        time.sleep(dice*3)\n",
    "        actions_pagedown_.perform()\n",
    "        if dice <0.2:\n",
    "            time.sleep(dice*random.choice(range(4,8)))\n",
    "        \n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        date = soup.find_all(class_ = 'section-review-publish-date')\n",
    "\n",
    "        if ('months' or 'year' or 'years') in ' '.join([j for i in date for j in i.contents]):\n",
    "\n",
    "            break\n",
    "    return driver.page_source\n",
    "\n",
    "def decode_html_date(page_source):\n",
    "    soup = BeautifulSoup(page_source,'html.parser')\n",
    "    \n",
    "    res_stars = soup.find_all(class_ = 'section-review-stars')\n",
    "    \n",
    "    res = soup.find_all(class_ = 'section-review-text')\n",
    "    \n",
    "    date = soup.find_all(class_ = 'section-review-publish-date')\n",
    "    \n",
    "    data = pd.DataFrame(columns=['text','label','date'])\n",
    "\n",
    "    text = []\n",
    "    jstcache = res[0].attrs['jstcache']\n",
    "    for i in res:\n",
    "\n",
    "        if i.attrs['jstcache'] == jstcache:\n",
    "            \n",
    "            if len(i.contents) > 0:\n",
    "                text.append(i.contents[0])\n",
    "            else:text.append(' ')\n",
    "\n",
    "    labels = []\n",
    "    for i in res_stars:\n",
    "        labels.append(i.attrs['aria-label'][1])\n",
    "    \n",
    "    date_list = []\n",
    "    for i in date:\n",
    "        date_list.append(i.contents[0])\n",
    "\n",
    "    assert(len(text)==len(labels))\n",
    "\n",
    "    data.loc[:,'text'] = text\n",
    "\n",
    "    data.loc[:,'label'] = labels\n",
    "    \n",
    "    data.loc[:,'date'] = date_list\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(path+'\\metadata.csv') # reading metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define options and drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('prefs', {'intl.accept_languages': 'en,en_US'})\n",
    "driver = webdriver.Chrome(path+'\\chromedriver79.exe',options = options)\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### excecuting scarping for the whole comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for Hong Ning Clinic 康宁中医诊所\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "searching for New China 中华楼\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "searching for HOT POT Thai Cuisine\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "searching for Shake Shack Leicester Square\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "searching for Canton\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "searching for MW Buffet\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "searching for LEON\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "118",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 118",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-cd0a49610305>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m110\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m110\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'total_rating'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mplace_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'place_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://www.google.com/maps/place/?q=place_id:{0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplace_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1492\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1493\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1494\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    866\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    986\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m                 \u001b[0msection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m                 \u001b[1;31m# we have yielded a scalar ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1911\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1913\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no slices here, handle elsewhere'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3583\u001b[0m                                                       drop_level=drop_level)\n\u001b[0;32m   3584\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3585\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3587\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 118"
     ]
    }
   ],
   "source": [
    "for i in range(metadata.shape[0]):\n",
    "    if metadata.loc[i,'total_rating'] >= 5:\n",
    "        name = metadata.loc[i,'name']\n",
    "        place_id = metadata.loc[i,'place_id']\n",
    "        url = 'https://www.google.com/maps/place/?q=place_id:{0}'.format(place_id)\n",
    "        print('searching for {0}'.format(name))\n",
    "        html = scrape_month(url,driver,place_id)\n",
    "        print('decoding data')\n",
    "        data = decode_html_date(html)\n",
    "        data.to_csv(path+'\\Monthly comments\\{0}.csv'.format(name.replace('/','').replace('|','').replace('<','').replace('>','')),\n",
    "                    index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### excecuting scraping for monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for Bun House\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Bun House\n",
      "searching for C & R Cafe Restaurant\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving C & R Cafe Restaurant\n",
      "searching for Caffe Concerto - Shaftesbury Avenue\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Caffe Concerto - Shaftesbury Avenue\n",
      "searching for Cafe TPT\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Cafe TPT\n",
      "searching for Chinatown Bakery\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Chinatown Bakery\n",
      "searching for Chinatown Gate\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Chinatown Gate\n",
      "searching for Dumplings' Legend\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Dumplings' Legend\n",
      "searching for Doughnut Time\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Doughnut Time\n",
      "searching for Golden Dragon Chinatown\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Golden Dragon Chinatown\n",
      "searching for Golden Phoenix\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Golden Phoenix\n",
      "searching for Feng Shui Inn\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Feng Shui Inn\n",
      "searching for Gerrard's Corner\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Gerrard's Corner\n",
      "searching for Good Friend Chicken\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Good Friend Chicken\n",
      "searching for Haozhan\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Haozhan\n",
      "searching for Hong Kong Buffet\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Hong Kong Buffet\n",
      "searching for ICHIBUNS SOHO\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving ICHIBUNS SOHO\n",
      "searching for Hovarda\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Hovarda\n",
      "searching for Jen Cafe\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Jen Cafe\n",
      "searching for Imperial China\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Imperial China\n",
      "searching for Lido\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Lido\n",
      "searching for Le Hanoi\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Le Hanoi\n",
      "searching for Little Korea Restaurant\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Little Korea Restaurant\n",
      "searching for Mamasons Dirty Ice cream (Chinatown London)\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Mamasons Dirty Ice cream (Chinatown London)\n",
      "searching for Lotus Garden\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Lotus Garden\n",
      "searching for London Chinatown Restaurant\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving London Chinatown Restaurant\n",
      "searching for Misato\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Misato\n",
      "searching for Mr Wu\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Mr Wu\n",
      "searching for New Loon Fung Restaurant\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving New Loon Fung Restaurant\n",
      "searching for Old Town 97\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Old Town 97\n",
      "searching for Olle Korean Barbecue\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Olle Korean Barbecue\n",
      "searching for Pho & Bun\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Pho & Bun\n",
      "searching for Orient London\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Orient London\n",
      "searching for Plum Valley Chinese Restaurant\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Plum Valley Chinese Restaurant\n",
      "searching for SHIBUYA SOHO\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving SHIBUYA SOHO\n",
      "searching for Tao Tao Ju\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Tao Tao Ju\n",
      "searching for Thai Tho Soho\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Thai Tho Soho\n",
      "searching for The Palomar\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving The Palomar\n",
      "searching for Tokyo Diner\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Tokyo Diner\n",
      "searching for Viet Food\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Viet Food\n",
      "searching for Wan Chai Corner\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Wan Chai Corner\n",
      "searching for Wong Kei\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Wong Kei\n",
      "searching for XU Teahouse & Restaurant\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving XU Teahouse & Restaurant\n",
      "searching for Young Cheng\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Young Cheng\n",
      "searching for Little Four Seasons\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Little Four Seasons\n",
      "searching for Chatime\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Chatime\n",
      "searching for Rasa Sayang Restaurant\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Rasa Sayang Restaurant\n",
      "searching for Cuppacha\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Cuppacha\n",
      "searching for De Hems Dutch Cafe Bar\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving De Hems Dutch Cafe Bar\n",
      "searching for Experimental Cocktail Club Chinatown\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Experimental Cocktail Club Chinatown\n",
      "searching for Happy Lemon\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Happy Lemon\n",
      "searching for Ku Bar\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Ku Bar\n",
      "searching for Loon Fung Chinatown\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Loon Fung Chinatown\n",
      "searching for New Loon Moon Supermarket\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving New Loon Moon Supermarket\n",
      "searching for Opium Cocktail bar and Dim Sum Parlour\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Opium Cocktail bar and Dim Sum Parlour\n",
      "searching for SeeWoo\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving SeeWoo\n",
      "searching for Waxy's Little Sister\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Waxy's Little Sister\n",
      "searching for Beijing Dumpling\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Beijing Dumpling\n",
      "searching for Bubblewrap Waffle\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Bubblewrap Waffle\n",
      "searching for O'Neill's Wardour Street\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving O'Neill's Wardour Street\n",
      "searching for Slug & Lettuce Leicester Square\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Slug & Lettuce Leicester Square\n",
      "searching for The Rialto, Grosvenor Casino Piccadilly\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving The Rialto, Grosvenor Casino Piccadilly\n",
      "searching for Waxy O'Connor's\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Waxy O'Connor's\n",
      "searching for Café de Paris\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Café de Paris\n",
      "searching for New China 中华楼\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving New China 中华楼\n",
      "searching for HOT POT Thai Cuisine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving HOT POT Thai Cuisine\n",
      "searching for Shake Shack Leicester Square\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving Shake Shack Leicester Square\n",
      "searching for MW Buffet\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving MW Buffet\n",
      "searching for LEON\n",
      "finding \"see all reviews\"\n",
      "successfully found\n",
      "loading data\n",
      "decoding data\n",
      "saving LEON\n"
     ]
    }
   ],
   "source": [
    "for i in range(metadata.shape[0]):\n",
    "    if metadata.loc[i,'total_rating'] >= 300:\n",
    "        name = metadata.loc[i,'name']\n",
    "        place_id = metadata.loc[i,'place_id']\n",
    "        url = 'https://www.google.com/maps/place/?q=place_id:{0}'.format(place_id)\n",
    "        print('searching for {0}'.format(name))\n",
    "        html = scrape(url,driver,150)\n",
    "        print('decoding data')\n",
    "        data = decode_html(html)\n",
    "        print('saving {0}'.format(name))\n",
    "        data.to_csv(path+'\\Extra_comments\\{0}.csv'.format(name),\n",
    "                index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit() ## quiting the scraping process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
